{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VarunSKumar94/Data-science-and-AI-ML/blob/main/randomForestBrakeApplication/Braking_Prediction_Model_(Random_Forest_with_GridSearchCV).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# braking_prediction_model.ipynb\n",
        "\n",
        "# --- 1. Setup and Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        "    make_scorer\n",
        ")\n",
        "\n",
        "# --- 2. Synthetic Data Generation ---\n",
        "# This simulates vehicle telematics data for demonstration purposes.\n",
        "# In a real scenario, you would load your actual data here.\n",
        "\n",
        "print(\"--- Generating Synthetic Data ---\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of vehicles and trips\n",
        "num_vins = 5\n",
        "num_trips_per_vin = 3\n",
        "records_per_trip = 500 # Number of data points per trip\n",
        "\n",
        "all_data = []\n",
        "for i in range(num_vins):\n",
        "    vin = f'V{i+1:03d}'\n",
        "    for j in range(num_trips_per_vin):\n",
        "        trip_id = j + 1\n",
        "        start_time = datetime(2023, 1, 1, np.random.randint(0, 23), np.random.randint(0, 59))\n",
        "\n",
        "        for k in range(records_per_trip):\n",
        "            current_time = start_time + timedelta(seconds=k * np.random.uniform(0.9, 1.1))\n",
        "\n",
        "            # Simulate vehicle speed (km/h)\n",
        "            speed = max(0, 60 + 15 * np.sin(k / 50) + np.random.randn() * 10)\n",
        "\n",
        "            # Simulate brake events (imbalanced)\n",
        "            # Roughly 10-15% of brake events\n",
        "            brake_pressed = 0\n",
        "            if np.random.rand() < 0.1: # Base probability\n",
        "                # Increase probability if speed is high and decelerating\n",
        "                if k > 0 and speed < all_data[-1][3] * 0.9: # If current speed is significantly lower than previous\n",
        "                    if np.random.rand() < 0.7:\n",
        "                        brake_pressed = 1\n",
        "                elif np.random.rand() < 0.05: # Small chance even without sharp deceleration\n",
        "                    brake_pressed = 1\n",
        "\n",
        "            all_data.append([vin, trip_id, current_time, speed, brake_pressed])\n",
        "\n",
        "df = pd.DataFrame(all_data, columns=['VIN', 'Trips', 'GPS_TimeStamp_Local', 'VhclSpd', 'Brake_Pressed'])\n",
        "print(f\"Synthetic data generated: {df.shape[0]} rows.\")\n",
        "print(\"Sample of raw data:\")\n",
        "print(df.head())\n",
        "\n",
        "# --- 3. Feature Engineering ---\n",
        "# This section implements the feature engineering steps discussed.\n",
        "# It uses Pandas operations, assuming data fits in memory for this script.\n",
        "# For large datasets, these steps would be adapted to PySpark as discussed previously.\n",
        "\n",
        "print(\"\\n--- Starting Feature Engineering ---\")\n",
        "\n",
        "# Define max lags and leads for feature generation\n",
        "max_spd_lags = 12\n",
        "max_acc_lags = 2\n",
        "max_spd_leads = 5\n",
        "max_acc_leads = 3\n",
        "\n",
        "def apply_all_feature_engineering(df_group, spd_col, time_col, max_spd_lags, max_acc_lags, max_spd_leads, max_acc_leads):\n",
        "    \"\"\"\n",
        "    Applies all feature engineering steps to a single group (e.g., a single trip).\n",
        "    Calculates acceleration, and then generates lagged and lead features for both speed and acceleration.\n",
        "    \"\"\"\n",
        "    df_group = df_group.sort_values(by=time_col).copy()\n",
        "\n",
        "    # Calculate acceleration from speed\n",
        "    df_group['timediff'] = df_group[time_col].diff().dt.total_seconds()\n",
        "    # Convert speed diff from km/h to m/s, then divide by timediff and 9.81 m/s^2 for g's\n",
        "    df_group['acc_from_spd_g'] = (df_group[spd_col].diff() * 1000 / 3600) / (df_group['timediff'] * 9.81)\n",
        "    # Handle potential division by zero or infinite values\n",
        "    df_group['acc_from_spd_g'] = df_group['acc_from_spd_g'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Add lagged and lead features for VhclSpd\n",
        "    for i in range(1, max_spd_lags + 1):\n",
        "        df_group[f'{spd_col}_lag_{i}'] = df_group[spd_col].shift(i)\n",
        "    for i in range(1, max_spd_leads + 1):\n",
        "        df_group[f'{spd_col}_lead_{i}'] = df_group[spd_col].shift(-i) # Negative shift for leads\n",
        "\n",
        "    # Add lagged and lead features for acc_from_spd_g\n",
        "    # Ensure 'acc_from_spd_g' column exists before trying to shift it\n",
        "    if 'acc_from_spd_g' in df_group.columns:\n",
        "        for i in range(1, max_acc_lags + 1):\n",
        "            df_group[f'acc_from_spd_g_lag{i}'] = df_group['acc_from_spd_g'].shift(i)\n",
        "        for i in range(1, max_acc_leads + 1):\n",
        "            df_group[f'acc_from_spd_g_lead{i}'] = df_group['acc_from_spd_g'].shift(-i)\n",
        "    else:\n",
        "        # If 'acc_from_spd_g' was not created (e.g., group too short),\n",
        "        # create NaN columns for its lags/leads to avoid KeyError later\n",
        "        for i in range(1, max_acc_lags + 1):\n",
        "            df_group[f'acc_from_spd_g_lag{i}'] = np.nan\n",
        "        for i in range(1, max_acc_leads + 1):\n",
        "            df_group[f'acc_from_spd_g_lead{i}'] = np.nan\n",
        "\n",
        "    return df_group\n",
        "\n",
        "# Apply all feature engineering grouped by VIN and Trips in one go\n",
        "df_engineered = df.groupby(['VIN', 'Trips'], group_keys=False).apply(\n",
        "    lambda x: apply_all_feature_engineering(x, 'VhclSpd', 'GPS_TimeStamp_Local',\n",
        "                                            max_spd_lags, max_acc_lags, max_spd_leads, max_acc_leads)\n",
        ")\n",
        "print(df_engineered.columns)\n",
        "print(\"Feature engineering complete.\")\n",
        "print(\"Sample of engineered data (showing some new columns):\")\n",
        "print(df_engineered[['VhclSpd', 'acc_from_spd_g', 'VhclSpd_lag_1', 'VhclSpd_lead_1', 'acc_from_spd_g_lag1', 'acc_from_spd_g_lead1']].head())\n",
        "\n",
        "\n",
        "# Define feature columns for the model\n",
        "feature_cols = [\n",
        "    'VhclSpd', 'acc_from_spd_g'\n",
        "]\n",
        "# Add all generated lag and lead columns\n",
        "for i in range(1, max_spd_lags + 1):\n",
        "    feature_cols.append(f'VhclSpd_lag_{i}')\n",
        "for i in range(1, max_acc_lags + 1):\n",
        "    feature_cols.append(f'acc_from_spd_g_lag{i}')\n",
        "for i in range(1, max_spd_leads + 1):\n",
        "    feature_cols.append(f'VhclSpd_lead_{i}')\n",
        "for i in range(1, max_acc_leads + 1):\n",
        "    feature_cols.append(f'acc_from_spd_g_lead{i}')\n",
        "\n",
        "# Drop rows with any NaN values in the feature columns (due to lags/leads at group boundaries)\n",
        "df_final = df_engineered.dropna(subset=feature_cols).reset_index(drop=True)\n",
        "print(f\"\\nData after dropping NaNs: {df_final.shape[0]} rows.\")\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_final[feature_cols]\n",
        "y = df_final['Brake_Pressed']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "print(f\"Training target distribution: {Counter(y_train)}\")\n",
        "print(f\"Test target distribution: {Counter(y_test)}\")\n",
        "\n",
        "\n",
        "# --- 4. Data Scaling ---\n",
        "# Initialize and FIT the scaler on your TRAINING data\n",
        "print(\"\\n--- Starting Data Scaling ---\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "# Transform the test data using the *same* fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Data scaling complete.\")\n",
        "\n",
        "\n",
        "# --- 5. Model Training & Hyperparameter Tuning (Random Forest with GridSearchCV) ---\n",
        "print(\"\\n--- Starting Model Training & Hyperparameter Tuning ---\")\n",
        "\n",
        "# Define custom scorer for class 1 recall\n",
        "recall_class_1_scorer = make_scorer(recall_score, pos_label=1)\n",
        "\n",
        "scoring_metrics = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'recall_class_1': recall_class_1_scorer,\n",
        "    'f1_score': 'f1' # Adding F1-score for comprehensive evaluation\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced') # Use class_weight for imbalance\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "# This grid is a starting point and can be expanded or refined based on results.\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150], # Number of trees\n",
        "    'max_depth': [5, 10, 15],       # Max depth of each tree\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt', None] # Number of features to consider at each split\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_rf = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=3, # Reduced to 3 folds for faster demonstration, consider 5 or more for production\n",
        "    scoring=scoring_metrics,\n",
        "    refit='recall_class_1', # Refit the best model based on recall for class 1\n",
        "    verbose=2, # Detailed output during search\n",
        "    n_jobs=-1 # Use all available CPU cores for faster computation\n",
        ")\n",
        "\n",
        "print(\"Starting GridSearchCV for Random Forest...\")\n",
        "grid_search_rf.fit(X_train_scaled, y_train)\n",
        "print(\"GridSearchCV complete.\")\n",
        "\n",
        "# Get the best parameters and the best score\n",
        "print(f\"\\nBest parameters found: {grid_search_rf.best_params_}\")\n",
        "print(f\"Best cross-validation 'recall_class_1' score: {grid_search_rf.best_score_:.4f}\")\n",
        "\n",
        "# Store the best estimator\n",
        "best_rf_model_tuned = grid_search_rf.best_estimator_\n",
        "\n",
        "# --- 6. Model Evaluation ---\n",
        "print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
        "\n",
        "y_pred_proba_rf = best_rf_model_tuned.predict_proba(X_test_scaled)[:, 1] # Probabilities for class 1.0\n",
        "\n",
        "# Evaluate different prediction thresholds\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "best_recall_target = 0 # To track the best recall achieved\n",
        "best_threshold_found = 0.5 # Default threshold\n",
        "best_precision_at_threshold = 0\n",
        "\n",
        "print(\"\\nEvaluating different prediction thresholds:\")\n",
        "for t in thresholds:\n",
        "    y_pred_tuned_threshold = (y_pred_proba_rf >= t).astype(int)\n",
        "    current_recall = recall_score(y_test, y_pred_tuned_threshold, pos_label=1)\n",
        "    current_precision = precision_score(y_test, y_pred_tuned_threshold, pos_label=1)\n",
        "    current_f1 = f1_score(y_test, y_pred_tuned_threshold, pos_label=1)\n",
        "    current_accuracy = accuracy_score(y_test, y_pred_tuned_threshold)\n",
        "\n",
        "    print(f\"Threshold: {t:.2f} | Recall (1): {current_recall:.4f} | Precision (1): {current_precision:.4f} | F1 (1): {current_f1:.4f} | Accuracy: {current_accuracy:.4f}\")\n",
        "\n",
        "    # Logic to select the best threshold: prioritize recall >= 0.85, then maximize precision/F1\n",
        "    if current_recall >= 0.85: # Your target recall\n",
        "        if current_precision > best_precision_at_threshold: # Maximize precision among those meeting recall target\n",
        "            best_recall_target = current_recall\n",
        "            best_precision_at_threshold = current_precision\n",
        "            best_threshold_found = t\n",
        "    elif current_recall > best_recall_target and best_recall_target < 0.85: # If not yet at target, just find highest recall\n",
        "        best_recall_target = current_recall\n",
        "        best_precision_at_threshold = current_precision\n",
        "        best_threshold_found = t\n",
        "\n",
        "\n",
        "print(f\"\\nOptimal Threshold for Recall >= 0.85: {best_threshold_found:.2f}\")\n",
        "print(f\"Achieved Recall (Class 1) at this threshold: {recall_score(y_test, (y_pred_proba_rf >= best_threshold_found).astype(int), pos_label=1):.4f}\")\n",
        "print(f\"Achieved Precision (Class 1) at this threshold: {precision_score(y_test, (y_pred_proba_rf >= best_threshold_found).astype(int), pos_label=1):.4f}\")\n",
        "print(f\"Achieved F1-Score (Class 1) at this threshold: {f1_score(y_test, (y_pred_proba_rf >= best_threshold_found).astype(int), pos_label=1):.4f}\")\n",
        "\n",
        "final_y_pred_test = (y_pred_proba_rf >= best_threshold_found).astype(int)\n",
        "print(\"\\nFinal Classification Report on Test Set:\")\n",
        "print(confusion_matrix(y_test, final_y_pred_test))\n",
        "print(f\"Final Accuracy: {accuracy_score(y_test, final_y_pred_test):.4f}\")\n",
        "\n",
        "\n",
        "# --- 7. Model and Scaler Serialization ---\n",
        "print(\"\\n--- Saving Model and Scaler ---\")\n",
        "# Save both the trained model and the FITTED scaler\n",
        "with open('random_forest_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_rf_model_tuned, model_file)\n",
        "\n",
        "with open('scaler.pkl', 'wb') as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file) # Save the FITTED scaler!\n",
        "\n",
        "print(\"Model and Scaler saved as 'random_forest_model.pkl' and 'scaler.pkl'.\")\n",
        "\n",
        "\n",
        "# --- 8. Prediction on New Unseen Data (Deployment Simulation) ---\n",
        "print(\"\\n--- Simulating Prediction on New Unseen Data ---\")\n",
        "\n",
        "# Create a dummy new unseen DataFrame (similar structure to original raw data)\n",
        "new_unseen_data = []\n",
        "new_vin = 'V999'\n",
        "new_trip_id = 1\n",
        "new_start_time = datetime(2024, 6, 1, 10, 0)\n",
        "for k in range(100): # 100 new data points\n",
        "    current_time = new_start_time + timedelta(seconds=k * np.random.uniform(0.9, 1.1))\n",
        "    speed = max(0, 70 + 10 * np.cos(k / 20) + np.random.randn() * 5)\n",
        "    new_unseen_data.append([new_vin, new_trip_id, current_time, speed, 0]) # Brake_Pressed is unknown initially\n",
        "\n",
        "new_unseen_df_raw = pd.DataFrame(new_unseen_data, columns=['VIN', 'Trips', 'GPS_TimeStamp_Local', 'VhclSpd', 'Brake_Pressed_Actual'])\n",
        "\n",
        "print(\"New unseen raw data sample:\")\n",
        "print(new_unseen_df_raw.head())\n",
        "\n",
        "# Apply the same feature engineering steps to the new unseen data\n",
        "new_unseen_df_fe = new_unseen_df_raw.groupby(['VIN', 'Trips'], group_keys=False).apply(\n",
        "    lambda x: apply_all_feature_engineering(x, 'VhclSpd', 'GPS_TimeStamp_Local',\n",
        "                                            max_spd_lags, max_acc_lags, max_spd_leads, max_acc_leads)\n",
        ")\n",
        "\n",
        "\n",
        "# Select only the feature columns and drop NaNs\n",
        "X_new_unseen_features = new_unseen_df_fe[feature_cols].dropna().reset_index(drop=True)\n",
        "original_indices_for_mapping = X_new_unseen_features.index # Keep original indices to map back\n",
        "\n",
        "# Load the saved model and scaler\n",
        "with open('random_forest_model.pkl', 'rb') as model_file:\n",
        "    loaded_model = pickle.load(model_file)\n",
        "with open('scaler.pkl', 'rb') as scaler_file:\n",
        "    loaded_scaler = pickle.load(scaler_file)\n",
        "\n",
        "# Scale the new unseen data using the *loaded* scaler\n",
        "X_new_unseen_scaled = loaded_scaler.transform(X_new_unseen_features)\n",
        "\n",
        "# Get probabilities from the loaded model\n",
        "y_pred_proba_new_unseen = loaded_model.predict_proba(X_new_unseen_scaled)[:, 1]\n",
        "\n",
        "# Apply the optimal threshold found during training\n",
        "y_pred_new_unseen = (y_pred_proba_new_unseen >= best_threshold_found).astype(int)\n",
        "\n",
        "# Map predictions back to a DataFrame with original identifiers\n",
        "predictions_output = pd.DataFrame({\n",
        "    'VIN': new_unseen_df_fe.loc[X_new_unseen_features.index, 'VIN'],\n",
        "    'Trips': new_unseen_df_fe.loc[X_new_unseen_features.index, 'Trips'],\n",
        "    'GPS_TimeStamp_Local': new_unseen_df_fe.loc[X_new_unseen_features.index, 'GPS_TimeStamp_Local'],\n",
        "    'Brake_Predicted': y_pred_new_unseen,\n",
        "    'Brake_Probability_Class1': y_pred_proba_new_unseen\n",
        "})\n",
        "\n",
        "print(\"\\nNew Unseen Data with Predictions:\")\n",
        "print(predictions_output.head())\n",
        "print(f\"\\nPredicted Brake_Pressed counts for new data: {Counter(predictions_output['Brake_Predicted'])}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wFql-zZfqbAK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}